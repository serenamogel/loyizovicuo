<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Tremont: A Wider Front End and Caches -</title><meta name=robots content="index,follow,noarchive"><meta name=description content="For users that have been following our analysis of the Core microarchitecture, it has been hard not to notice that Intel’s design for that family has been, among other things, to continually go wider and wider. This means more instructions in flight, larger caches, bigger buffers, bigger TLBs, more execution ports, and support for more instructions. Going wider isn’t the only thing: the microarchitecture also has to be clever, trying to maximise utilisation, as going wider does nothing for a simple stream of commands."><meta name=author content="Aldo Pusey"><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/app.css><link rel="preload stylesheet" as=style href=https://assets.cdnweb.info/hugo/paper/css/an-old-hope.min.css><script defer src=https://assets.cdnweb.info/hugo/paper/js/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=./theme.png><link rel=icon href=./favicon.ico><link rel=apple-touch-icon href=./apple-touch-icon.png><meta name=generator content="Hugo 0.98.0"><meta property="og:title" content="Tremont: A Wider Front End and Caches"><meta property="og:description" content="For users that have been following our analysis of the Core microarchitecture, it has been hard not to notice that Intels design for that family has been, among other things, to continually go wider and wider. This means more instructions in flight, larger caches, bigger buffers, bigger TLBs, more execution ports, and support for more"><meta property="og:type" content="article"><meta property="og:url" content="/intels-new-atom-microarchitecture-the-tremont-core.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-10-09T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-09T00:00:00+00:00"><meta itemprop=name content="Tremont: A Wider Front End and Caches"><meta itemprop=description content="For users that have been following our analysis of the Core microarchitecture, it has been hard not to notice that Intels design for that family has been, among other things, to continually go wider and wider. This means more instructions in flight, larger caches, bigger buffers, bigger TLBs, more execution ports, and support for more"><meta itemprop=datePublished content="2024-10-09T00:00:00+00:00"><meta itemprop=dateModified content="2024-10-09T00:00:00+00:00"><meta itemprop=wordCount content="1115"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Tremont: A Wider Front End and Caches"><meta name=twitter:description content="For users that have been following our analysis of the Core microarchitecture, it has been hard not to notice that Intels design for that family has been, among other things, to continually go wider and wider. This means more instructions in flight, larger caches, bigger buffers, bigger TLBs, more execution ports, and support for more"></head><body class=not-ready data-menu=true><header class=header><p class=logo><a class=site-name href=./index.html>DashJ</a><a class=btn-dark></a></p><script>let bodyClx=document.body.classList,btnDark=document.querySelector(".btn-dark"),sysDark=window.matchMedia("(prefers-color-scheme: dark)"),darkVal=localStorage.getItem("dark"),setDark=e=>{bodyClx[e?"add":"remove"]("dark"),localStorage.setItem("dark",e?"yes":"no")};setDark(darkVal?darkVal==="yes":sysDark.matches),requestAnimationFrame(()=>bodyClx.remove("not-ready")),btnDark.addEventListener("click",()=>setDark(!bodyClx.contains("dark"))),sysDark.addEventListener("change",e=>setDark(e.matches))</script><nav class=menu><a href=./sitemap.xml>Sitemap</a></nav></header><main class=main><article class=post-single><header class=post-title><p><time>Oct 9, 2024</time>
<span>Aldo Pusey</span></p><h1>Tremont: A Wider Front End and Caches</h1></header><section class=post-content><p>For users that have been following our analysis of the Core microarchitecture, it has been hard not to notice that Intel’s design for that family has been, among other things, to continually go wider and wider. This means more instructions in flight, larger caches, bigger buffers, bigger TLBs, more execution ports, and support for more instructions. Going wider isn’t the only thing: the microarchitecture also has to be clever, trying to maximise utilisation, as going wider does nothing for a simple stream of commands.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15009/Tremont%20-%20Stephen%20Robinson%20-%20Linley%20-%20Final-page-004_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>With Atom, going wider is a key part of the design for Tremont, but Intel has taken a couple of fundamentally different steps to manage exactly what is going on.</p><h3>A Key Uplift: Fetch and Predict</h3><p>Another major jump for the Atom microarchitecture are the prefetchers and branch predictors. Intel states that rather than iterate the design from Goldmont Plus, they have transplanted a large part of the prefetchers and branch predictors from the Core design on Sunny Cove. The design isn’t identical according to Intel, due to die area and power concerns, but Intel states that the principles are similar and elements like branch prediction history tables are ‘of the same order’ as the Core design.</p><p align=center><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15009/Tremont%20-%20Stephen%20Robinson%20-%20Linley%20-%20Final-page-005_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Intel states that there is no penalty for an L1 prediction, and that the L2 prediction penalty is smaller than previous generations.&nbsp;</p><h3>A Key Differentiator: Decode Engines</h3><p>On the face of it, we have a 6-wide decode engine pared with a 4-wide allocation/dispatch engine. This is, on paper, very odd: normally we would expect the decode and dispatch to be equal in width, or at least be able to dispatch more than can be decoded in order to ensure that the re-order buffer doesn’t overflow. With the latest Core microarchitecture, called Sunny Cove, we have a 4-to-6 wide decode that also supports a micro-op cache, which all leads into a large reorder buffer and a 10-wide dispatch to the back-end. Tremont is, by contrast, has the opposite ratio.</p><p>Saying that this is a 6-wide decode engine is actually slightly incorrect. What Intel has done here is split the decode into dual 3-wide engines instead.</p><p><a href=#><img alt src=https://cdn.statically.io/img/images.anandtech.com/doci/15009/Tremont%20-%20Stephen%20Robinson%20-%20Linley%20-%20Final-page-006_575px.jpg style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></a></p><p>Each decode engine, when dealing with different branch predictions, can take a separate instruction stream. This allows for a higher average utilization across both of the 3-wide decode engines compared to a single 6-wide engine, but when a branch isn’t present it means that one of the decode engines can be clock gated to save power. For a single instruction stream, the Tremont design is actually only 3-wide decode, with a 4-wide dispatch.</p><p>(Technically Intel states that, through microcode, they can change the decode engines to act as a single 6-wide implementation rather than dual 3-wide engines. This won’t be configurable to the OEM, but based on demand Intel may make specific products for customers that request it.)</p><p>So just to clarify, Tremont does not have a micro-op cache. When discussing with Intel about the benefits of this dual decode engine design compared to having a micro-op cache, Intel stated that a micro-op cache can help utilize a wide-decode design better, but with a smaller per-engine decode size, they were able to see a performance uplift as well as save die area by using this dual-engine design. Intel declined to comment which one was better, but we were told that given the die size, power envelope of Atom, and the typical instruction flow of an Atom core, this design yielded the better combination of performance, power, and area.</p><p>Another improvement for Intel after the decode engines is the re-order buffer. Intel states that it can support 208 instructions, compared to <a href=#>78 in Goldmont and 95 in Goldmont Plus</a>, which is a sizeable uplift. Intel did not specify if Tremont has the ability to fuse instructions into micro-ops for the ROB (Goldmont did not), however there is a near 1:1 parity of instructions to micro-ops we were told.</p><h3>Caches</h3><p>Intel has also increased the size of its L1 data cache. The L1 instruction cache says at 32 KiB/core with 8-way associativity, but ever since the 22nm era, Intel has kept a 24 KiB/core L1 data cache on its Atom design. With Tremont, both the L1-I and L1-D are now a 32 KiB/core design with 8-way associativity. Intel states that its L1 data cache here has a 3-cycle latency, compared to Skylake which has a 32 KiB L1D at a 4-cycle latency, or Sunny Cove which has a 48 KiB L1D at a 5-cycle latency.</p><table border=0 width=95%><tbody readability=2><tr class=tgrey><td colspan=7>Intel Caches</td></tr><tr class=tlblue><td><i>AnandTech</i></td><td>Tremont</td><td>Goldmont+</td><td>Goldmont</td><td>&nbsp;</td><td>Sunny Cove</td><td>Skylake</td></tr><tr><td class=tlgrey>Process</td><td>10+</td><td>14</td><td>14</td><td>&nbsp;</td><td>10+</td><td>14++</td></tr><tr><td class=tlgrey>Decode</td><td>2x3-wide</td><td>3-wide</td><td>3-wide</td><td>&nbsp;</td><td>4-6 wide</td><td>4-5 wide</td></tr><tr><td class=tlgrey>Allocate</td><td>4-wide</td><td>4-wide</td><td>3-wide</td><td>&nbsp;</td><td>10-wide</td><td>8-wide</td></tr><tr><td class=tlgrey>L1 Instruction</td><td>32 KiB/Core<br>8-way</td><td>32 KiB/Core<br>8-way</td><td>32 KiB/Core<br>8-way</td><td>&nbsp;</td><td>32 KiB/Core<br>8-way</td><td>32 KiB/Core<br>8-way</td></tr><tr><td class=tlgrey>L1 Data</td><td>32 KiB/Core<br>8-way</td><td>24 KiB/Core<br>6-way</td><td>24 KiB/Core<br>6-way</td><td>&nbsp;</td><td>48 KiB/Core<br>12-way</td><td>32 KiB/Core<br>8-way</td></tr><tr><td class=tlgrey>L1 Latency</td><td>3-cycle</td><td>3-cycle</td><td>3-cycle</td><td>&nbsp;</td><td>5-cycle</td><td>4-cycle</td></tr><tr readability=4><td class=tlgrey>L2 Cache</td><td>1.5-4.5 MiB<br>Per Module<br>12-18 way</td><td>1.0 MiB<br>Per Core<br>16-way</td><td>0.5-1.0 MiB<br>Per Core<br>16-way</td><td>&nbsp;</td><td>512 KiB<br>Per Core<br>8-way&nbsp;</td><td>256 KiB<br>Per Core<br>4-way</td></tr><tr><td class=tlgrey>L2 Latency</td><td>17-cycle</td><td>19-cycle</td><td>17-cycle</td><td>&nbsp;</td><td>13-cycle</td><td>12 cycle</td></tr></tbody></table><p>For the L2 cache, for most Atom cores in the past, this has essentially been a last-level cache split across all cores in a ‘module’. Depending on the generation will depend on the size of the module: for 22nm Silvermont, we saw an L2 cache of 512 KiB/core, which was increased with Goldmont up to 1.0 MB/core. With Tremont, Intel has specified that L2 will vary depending on the product, from 1.5 MiB to 4.5 MiB per module. A module for Tremont will be anything up to four cores, so we could see designs with a single core featuring 4.5 MiB of L2 cache, or a quad-core design with 1.5 MiB of L2. Within a module, all cores have access to the cache, however a core will not have access to the L2 in a different module. The L2 can be set through microcode as an inclusive or a non-inclusive cache.</p><p>Intel states that the L2 cache has an average 17-cycle latency, and the associativity will be a function of the size: 1.5 MB will be a 12-way design, while 4.5 MB will be an 18-way design. (We therefore assume that a 3.0 MB L2 will be 15-way.)</p><p>Tremont also adds support for a global L3 cache across modules. Both the L2 and L3 cache support QoS arrangements, allowing for data prioritization and memory bandwidth enforcement for specific threads or virtual machines. This is a technology that Intel introduced with Broadwell Xeons to help avoid ‘noisy neighbor’ environments in data centers.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH52fI9yZqKmpJq5tHnNnq5mmaSkum65yJypqJmimLWqwMScq66qlWLBqbGMramepZ%2BjwW6vzqucaGo%3D</p></section><nav class=post-nav><a class=prev href=./peter-mcmahon-biography.html><span>←</span><span>Peter McMahon Bio (Wiki)</span></a>
<a class=next href=./how-eat-string-cheese.html><span>5 Ways to Eat String Cheese</span><span>→</span></a></nav></article></main><footer class=footer><p>&copy; 2024 <a href=./></a></p><p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>